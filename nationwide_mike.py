# -*- coding: utf-8 -*-
"""nationwide_mike

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEp_uERXaTGjbpyZyK5QHMh43V-Vgnn2
"""

import pandas as pd
import os
import re
!pip install googletrans==4.0.0-rc1
from googletrans import Translator

dataset = '/content/Nationwide.csv'

nationwide = pd.read_csv(dataset, low_memory=True)
nationwide.info()
nationwide.head()

# Translate headers to english
translator = Translator()

new_headers = []
for header in nationwide.columns:
  try:
    translation = translator.translate(header, dest='en')
    new_headers.append(translation.text)
  except:
    new_headers.append(header)

nationwide.columns = new_headers
print(nationwide.columns)

# Save translated csv
path = 'nationwide'

if not os.path.exists(path):
  os.makedirs(path)

nationwide.to_csv('/content/nationwide/nationwide_english.csv', index=False)

nationwide = pd.read_csv('/content/nationwide/nationwide_english.csv')
nationwide.info()
nationwide.head()

# Normalize headers
def normalize_header(header):
  header = header.lower()
  header = header.replace(' ', '_')
  header = re.sub(r'[^a-zA-Z0-9_]', '', header)
  return header


new_headers = [normalize_header(header) for header in nationwide.columns]
nationwide.columns = new_headers

print(nationwide.columns)

# Save translated csv
nationwide.to_csv('/content/nationwide/nationwide_normalized.csv', index=False)

nationwide = pd.read_csv('/content/nationwide/nationwide_normalized.csv')
nationwide.info()
nationwide.head()

# Find duplicate rows based on 'frame_number', 'id_card', 'mail' and 'motor_number'
duplicate_rows = nationwide[nationwide.duplicated(subset=[
    'frame_number',
    'id_card',
    'mail',
    'motor_number'
], keep=False)]

# Drop duplicates, keeping the first occurrence
nationwide.drop_duplicates(subset=[
    'frame_number',
    'id_card',
    'mail',
    'motor_number'
], keep='first', inplace=True)

# Save the duplicates and unique rows to separate CSV files
duplicate_rows.to_csv('/content/nationwide/nationwide_dump.csv', index=False)
nationwide.to_csv('/content/nationwide/nationwide_en_single.csv', index=False)

nationwide = pd.read_csv('/content/nationwide/nationwide_en_single.csv')
nationwide.info()

# Drop columns
columns_to_drop = ['gender', 'industry', 'monthly_salary',
                   'marriage', 'educate', 'brand',
                   'car', 'model', 'configuration',
                   'color', 'unnamed_21']

nationwide = nationwide.drop(columns=columns_to_drop, errors='ignore')
nationwide.to_csv('/content/nationwide/nationwide_key.csv', index=False)

nationwide = pd.read_csv('/content/nationwide/nationwide_key.csv')
nationwide.info()

# Split dataset into 5 chunks
def split_dataframe(df, num_chunks):
  """Splits a DataFrame into a specified number of chunks."""
  chunk_size = len(df) // num_chunks
  chunks = []
  for i in range(num_chunks):
    start_index = i * chunk_size
    end_index = (i + 1) * chunk_size if i < num_chunks - 1 else len(df)
    chunks.append(df.iloc[start_index:end_index])
  return chunks

# Create the 'chunks' directory if it doesn't exist
if not os.path.exists('chunks'):
    os.makedirs('chunks')

# Split the DataFrame into 5 chunks
num_chunks = 5
chunks = split_dataframe(nationwide, num_chunks)

# Save each chunk to a separate CSV file in the 'chunks' directory
for i, chunk in enumerate(chunks):
    chunk.to_csv(f'chunks/valid_{i+1}.csv', index=False)

# Validate email addresses
def validate_email(email):
    """Validates an email address using a regular expression."""
    if pd.isnull(email):
        return True  # Treat missing values as valid for now
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return bool(re.fullmatch(pattern, email))

# Define input and output directories
chunks_dir = 'chunks'
valid_chunks_dir = 'valid_chunks'
invalid_chunks_dir = 'invalid_chunks'

# Create output directories if they don't exist
if not os.path.exists(valid_chunks_dir):
    os.makedirs(valid_chunks_dir)
if not os.path.exists(invalid_chunks_dir):
    os.makedirs(invalid_chunks_dir)

# Loop through each file in the chunks directory
for filename in os.listdir(chunks_dir):
    if filename.endswith('.csv'):
        file_path = os.path.join(chunks_dir, filename)
        # Load the chunk
        chunk = pd.read_csv(file_path)

        # Validate the 'mail' column in the chunk
        chunk['mail_valid'] = chunk['mail'].apply(validate_email)

        # Separate valid and invalid rows
        valid_chunk = chunk[chunk['mail_valid']]
        invalid_chunk = chunk[~chunk['mail_valid']]

        # Save valid chunk to 'clean_chunks' folder
        if not valid_chunk.empty:
            valid_chunk = valid_chunk.drop('mail_valid', axis=1)
            valid_chunk.to_csv(os.path.join(valid_chunks_dir, f'valid_{filename.split("_")[-1]}'), index=False)

        # Save invalid chunk to 'invalid' folder
        if not invalid_chunk.empty:
            invalid_chunk = invalid_chunk.drop('mail_valid', axis=1)
            invalid_chunk.to_csv(os.path.join(invalid_chunks_dir, f'invalid_{filename.split("_")[-1]}'), index=False)

print("Chunks processed. Valid chunks saved to 'clean_chunks' folder, invalid chunks to 'invalid' folder.")

valid = pd.read_csv('/content/valid_chunks/valid_1.csv')
valid.info()

invalid = pd.read_csv('/content/invalid_chunks/invalid_1.csv')
invalid.info()

# Concatenate chunks to clean and invalid csv
def combine_chunks(input_dir, output_file):
    """Combines all CSV files in a directory into a single CSV file."""
    all_chunks = []
    for filename in os.listdir(input_dir):
        if filename.endswith('.csv'):
            file_path = os.path.join(input_dir, filename)
            chunk = pd.read_csv(file_path)
            all_chunks.append(chunk)

    if all_chunks:
        combined_df = pd.concat(all_chunks, ignore_index=True)
        # Create the output directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True) # This line was added to create the directory
        combined_df.to_csv(output_file, index=False)
        print(f"Combined chunks from '{input_dir}' to '{output_file}'.")
    else:
        print(f"No CSV files found in '{input_dir}'.")


# Combine clean chunks
combine_chunks('valid_chunks', 'valid_csv/Nationwide_clean.csv')

# Combine invalid chunks
combine_chunks('invalid_chunks', 'invalid_csv/Natinwide_invalid.csv')